{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58a4ca5-41ad-4ea0-95c9-756e609497bc",
   "metadata": {},
   "source": [
    "# \"Revisiting AlexNet\"\n",
    "> \"Awesome summary of the novel paper that started the deep learning big bang\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/chef_bepita.jpg\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37d1ed-7cbd-4617-b6a9-4b70923464f3",
   "metadata": {},
   "source": [
    "## Architecture Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e4a6b-8a09-4537-bf23-eec0b0fa8f60",
   "metadata": {},
   "source": [
    "### ReLU Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99df09f-4751-4d1c-a1ef-1db9a7acc28d",
   "metadata": {},
   "source": [
    "The first innovation that the paper brings to the table is the now most common ReLU ($ f(x) = max(0,x) $) activation function. Till then the most common way of applying non linearity to neural networks was the tanh ( $ f(x) = (1 + e^{-x})^{-1} $) activation function. Actually ReLUs were first ??? introduced by Nair and Hinton[???] but it was in the context of restricted Boltzman machines ???. The advantage of ReLU over tanh is that ReLU does not saturate and even if some fraction of the current batch will produce a positive input learning will occur in that pass. Therefore training a deep neural network with ReLUs is considerably faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7ddcc-3426-4eda-bbc5-bc7adb08103b",
   "metadata": {},
   "source": [
    "![ReLU vs. tanh training pace](../images/ReLU_vs_tanh_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b765d-5b12-408e-aa71-2836ef38d425",
   "metadata": {},
   "source": [
    "### Training on multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822662c-e734-47d0-a20b-7206b7eb6739",
   "metadata": {},
   "source": [
    "Back in the old days of 2011-2012 RTX2080 Ti were not available. The GPU they had was GTX 580 with 3GB of memory. But AlexNet implemntation taks advantage of the ability of GPUs to cross parrallelize the training. So AlexNet was trained on a pair of GTX 580 and the network architecture was designed for that purpase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44e24b-ac27-472b-bfd5-390500d7c323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a183b9-448b-4128-9ebb-54090b2c6619",
   "metadata": {},
   "source": [
    "### Local Response Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcad73-79ed-4d57-8faf-ab39ddee98d1",
   "metadata": {},
   "source": [
    "### Overlapping Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25d64c-bf7c-4c21-9fec-012e510dff83",
   "metadata": {},
   "source": [
    "## Reduce Overfitting Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadc4e4-9255-4c45-af43-f4aa2e23ab26",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081155b-79ea-43a6-a4dc-ff714aa9609f",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba3ec5-8e35-4a29-b451-c4101491475a",
   "metadata": {},
   "source": [
    "## Bibiliography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
